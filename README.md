# Дипломный практикум в Cloud: Amazon Web Services  
  
  
Полный текст задания приведен по [ссылке](https://github.com/als-port/devops-diplom).  
 
    
```text
Что необходимо для сдачи задания?

1. Репозиторий с конфигурационными файлами Terraform и готовность 
продемонстировать создание всех рессурсов с нуля.   
 ``` 
[Ссылка](https://github.com/als-port/dip-net/blob/main/terraform) на комплект фалов терраформ.

Попытался реализовать идею подъема (и настройки, где возможно и где хватает знаний) ВСЕЙ инфраструктуры требуемой по заданию, используя одну команду *terraform* *apply*. В целом, все получилось, но как всегда - есть пара нюансов.  
Итак, по приведенному набору скриптов поднимается все (вплоть до системы CI/CD (в моем случае Jenkins) с его частично настроенным агентом) по *terraform* *apply*, но за два приема, т.е. последовательность следующая:    

```shell
$ terraform apply
.. После ошибки по таймауту, на машине с которой производится подъем, необходимо выполнить 
.. обновление информации о конфигурации кластера eks с помощью команды:  
$ aws eks update-kubeconfig --name dip-net-cluster --region eu-central-1
.. Ну, и далее, снова:  
$ terraform apply
```
  
Осилить инкапсуляцию всей настройки Jenkins в ходе подъема инфраструктуры не удалось. Пришлось доконфигурировать руками.      
   
```text
2. Пример pull request с комментариями созданными atlantis'ом или снимки  
экрана из Terraform Cloud.  
```  
  
В качестве бекенда использовал S3 бакет.  
    
```text
3. Репозиторий с конфигурацией ansible, если был выбран способ создания Kubernetes 
кластера при помощи ansible.  
  ```

Кластер был организован не с помощью kubespray, а использовалось решение самого aws, т.е. eks (ранее уже немного работали с kubespray, решил немного потрогать руками новый инструмент). Все конфигурационные файлы там же:    

[Ссылка](https://github.com/als-port/dip-net/blob/main/terraform)  
  
```text
4. Репозиторий с Dockerfile тестового приложения и ссылка на  
собранный docker image.  
  ```

Приложение было решено оставить в этом же репозитории, находится в папке app:     

[Ссылка](https://github.com/als-port/dip-net/blob/main/app)

```text
Ссылка на собранный docker image:   
```
[https://hub.docker.com/r/alsxs/nginx/tags](https://hub.docker.com/r/alsxs/nginx/tags)
    
  
```text
5. Репозиторий с конфигурацией Kubernetes кластера.  
``` 
[Ссылка](https://github.com/als-port/dip-net/blob/main/terraform)  
  
```text
6. Ссылка на тестовое приложение и веб интерфейс Grafana с данными доступа.
```  
  
Ниже приведена картинка с поднятыми сущностями и ссылками, доступными из интернет на сервисы: приложение, Grafana, Jenkins:     
**Картинки сделаны, чтобы "влезло" по максимуму, поэтому. чтобы текст был читаемый, нужно на картинки нажимать, гитхаб в ридми качество немного упрощает**  
  
![Вывод](https://github.com/als-port/dip-net/blob/main/pics/cons_app_grafana_prometheus_jenkins.png)
  
Данные по доступу просты:  
login: admin, password: admin    

### Далее привожу ряд картинок с некоторыми комментариями  
  
  
Приложение, доступное извне по имени балансировщика:  
  
![Приложение](https://github.com/als-port/dip-net/blob/main/pics/app_lb.png)  
  
  

Prometheus, доступен через проброс порта:  
  
![Prometheus](https://github.com/als-port/dip-net/blob/main/pics/prometheus.png)  
  
  
  
Ниже дашборды Grafana (доступна извне по имени балансировщика сервиса). К сожалению, не реализовал развертывание в готовом виде, т.е. с настройками дашбордов. При этом, есть стандартные дашборды, которые очень быстро можно добавить через интерфейс самой Графаны (я использовал для примера стандартные, предложенные документацией: 3119, 6417):  
  
3119
![grafana3119](https://github.com/als-port/dip-net/blob/main/pics/grafana3119.png)  
  
6417
![grafana6417](https://github.com/als-port/dip-net/blob/main/pics/grafana6417_2.png)  
  
  
  
Jenkins, (доступен извне по имени балансировщика сервиса). Здесь, наверное, нужно сделать небольшие оговорки, как по конфигурации, так и по логике (эти решения немного отходят от общепринятой идеологии девопс, но были приняты для тестового стенда, для упрощения понимания и, чтобы было видно работу системы).  
1. При изначальном подъеме инфраструктуры, используются скрипты терраформа для развертывания приложения **kube_app.tf**. Но при завершении настройки системы CI/CD, Дженкинс после коммита передеплоит приложение с помощью манифеста **kube/kube_app.yaml**.  
2. Также (заведомо ошибочная настройка для prod, но не для stage) используется два образа для приложения: один исходный при начальном запуске, а второй обновляемый входе работы Дженкинса.   
Пайплайн приведен в jenkins/Jenkinsfile.  
3. Для приложения не использовался отдельный репозиторий. Соответственно реакция Дженкинса происходит и на коммиты не касающиеся изменения приложения. Я понимаю, что это не правильно и очень было бы легко создать дополнительный репозиторий. Сделал так только из понимания, что проект тестовый и, чтобы просто было все в одном месте.  
  
![Jenkins1](https://github.com/als-port/dip-net/blob/main/pics/jenkins_out_fin.png)  
  
Этапы  
![Jenkins2](https://github.com/als-port/dip-net/blob/main/pics/jenkins_stages.png)  
  
  
  
Дипломный проект выполнялся без использования кредов от Нетологии. Ранее переписывались с куратором курса по поводу проблем с ролями для eks. Данный нюанс был устранен, при этом, диалог велся во время 90%-й готовности работы и чтобы не тратить время на дополнительные тестирования, было принято завершить на личном аккаунте. В целом, хочу выразить благодарность за предоставление возможности выделения аккаунтов для дипломных проектов.    
Готов продемонстрировать работу всего, только прошу сообщить примерно за час-полтора, чтобы успел все поднять и настроить.  
  