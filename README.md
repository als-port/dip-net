# Дипломный практикум в Cloud: Amazon Web Services  
  
  
Полный текст задания приведен по [ссылке](https://github.com/als-port/devops-diplom).  
 
    
```text
Что необходимо для сдачи задания?

1. Репозиторий с конфигурационными файлами Terraform и готовность 
продемонстировать создание всех рессурсов с нуля.   
 ``` 
[Ссылка](https://github.com/als-port/dip-net/blob/main/terraform) на комплект фалов терраформ.

Попытался реализовать идею подъема (и настройки, где возможно и где хватает знаний) ВСЕЙ инфраструктуры требуемой по заданию (вплоть до системы CI/CD, в моем случае Jenkins), используя одну команду *terraform* *apply* (или по коммиту, если настроен Terraform Cloud). В целом, все получилось, но как всегда - есть пара нюансов.  

Осилить инкапсуляцию всей настройки Jenkins в ходе подъема инфраструктуры не удалось. Пришлось доконфигурировать руками.      
   
```text
2. Пример pull request с комментариями созданными atlantis'ом или снимки  
экрана из Terraform Cloud.  
```  
  
Изначально делал с s3 бакетом, но выпал момент деплоя атлантиса, поэтому в конце, чтобы не вносить больших изменений в рабочий набор скриптов, было принято решение переноса в Terraform Cloud. На картинке снизу один из последних действий по коммиту.  
  
![tcloud](https://github.com/als-port/dip-net/blob/main/pics/tcloud2.png) 
    
```text
3. Репозиторий с конфигурацией ansible, если был выбран способ создания Kubernetes 
кластера при помощи ansible.  
  ```

Кластер был организован не с помощью kubespray, а использовалось решение самого aws, т.е. eks (ранее уже немного работали с kubespray, решил немного потрогать руками новый инструмент). Все конфигурационные файлы там же:    

[Ссылка](https://github.com/als-port/dip-net/blob/main/terraform)  
  
```text
4. Репозиторий с Dockerfile тестового приложения и ссылка на  
собранный docker image.  
  ```

Приложение было решено оставить в этом же репозитории, находится в папке app:     

[Ссылка](https://github.com/als-port/dip-net/blob/main/app)

```text
Ссылка на собранный docker image:   
```
[https://hub.docker.com/r/alsxs/nginx/tags](https://hub.docker.com/r/alsxs/nginx/tags)
    
  
```text
5. Репозиторий с конфигурацией Kubernetes кластера.  
``` 
[Ссылка](https://github.com/als-port/dip-net/blob/main/terraform)  
  
```text
6. Ссылка на тестовое приложение и веб интерфейс Grafana с данными доступа.
```  
  
Ниже приведена картинка с поднятыми сущностями и ссылками, доступными из интернет на сервисы: приложение, Grafana, Jenkins:     
**Картинки сделаны, чтобы "влезло" по максимуму, поэтому. чтобы текст был читаемый, нужно на картинки нажимать, гитхаб в ридми качество немного упрощает**  
  
![Вывод](https://github.com/als-port/dip-net/blob/main/pics/cons_app_grafana_prometheus_jenkins.png)
  
Данные по доступу просты:  
login: admin, password: admin    

### Далее привожу ряд картинок с некоторыми комментариями  
  
  
Приложение, доступное извне по имени балансировщика:  
  
![Приложение](https://github.com/als-port/dip-net/blob/main/pics/app_lb.png)  
  
  

Prometheus, доступен через проброс порта:  
  
![Prometheus](https://github.com/als-port/dip-net/blob/main/pics/prometheus.png)  
  
  
  
Ниже дашборды Grafana (доступна извне по имени балансировщика сервиса). К сожалению, не реализовал развертывание в готовом виде, т.е. с настройками дашбордов. При этом, есть стандартные дашборды, которые очень быстро можно добавить через интерфейс самой Графаны (я использовал для примера стандартные, предложенные документацией: 3119, 6417):  
  
3119
![grafana3119](https://github.com/als-port/dip-net/blob/main/pics/grafana3119.png)  
  
6417
![grafana6417](https://github.com/als-port/dip-net/blob/main/pics/grafana6417_2.png)  
  
  
  
Jenkins, (доступен извне по имени балансировщика сервиса). Здесь, наверное, нужно сделать небольшие оговорки, как по конфигурации, так и по логике (эти решения немного отходят от общепринятой идеологии девопс, но были приняты для тестового стенда, для упрощения понимания и, чтобы было видно работу системы).  
1. При изначальном подъеме инфраструктуры, используются скрипты терраформа для развертывания приложения **kube_app.tf**. Но при завершении настройки системы CI/CD, Дженкинс после коммита передеплоит приложение с помощью манифеста **kube/kube_app.yaml**.  
2. Также (заведомо ошибочная настройка для prod, но не для stage) используется два образа для приложения: один исходный при начальном запуске, а второй обновляемый входе работы Дженкинса.   
Пайплайн приведен в jenkins/Jenkinsfile.  
3. Для приложения не использовался отдельный репозиторий. Соответственно реакция Дженкинса происходит и на коммиты не касающиеся изменения приложения. Я понимаю, что это не правильно и очень было бы легко создать дополнительный репозиторий. Сделал так только из понимания, что проект тестовый и, чтобы просто было все в одном месте.  
  
![Jenkins1](https://github.com/als-port/dip-net/blob/main/pics/jenkins_out_fin.png)  
  
Этапы  
![Jenkins2](https://github.com/als-port/dip-net/blob/main/pics/jenkins_stages_cr.png)  
  
  
  
Дипломный проект выполнялся без использования кредов от Нетологии. Ранее переписывались с куратором курса по поводу проблем с ролями для eks. Данный нюанс был устранен, при этом, диалог велся во время 90%-й готовности работы и чтобы не тратить время на дополнительные тестирования, было принято завершить на личном аккаунте. В целом, хочу выразить благодарность за предоставление возможности выделения аккаунтов для дипломных проектов.    
Готов продемонстрировать работу всего, только прошу сообщить примерно за час-полтора, чтобы успел все поднять и настроить.  
  
  
PS. Нужно признаться, есть момент, оставшийся некоторой **магией**. Дело в том что, условием работы модуля, который деплоит комплект Grafana и Prometheus, является наличие доступа уже к работающему кластеру. И соответственную ошибку я видел каждый деплой, то есть запускать нужно было в два раза:  
```shell
$ terraform apply  
.. После ошибки, на рабочей машине обновляем данные о кластере следующей командой:  
$ aws eks update-kubeconfig --name dip-net-cluster --region eu-central-1  
.. Ну, и далее, снова:  
$ terraform apply 
``` 
 
Но после установки и настройки модуля запускающего Jenkins ошибка исчезла и инфраструктура вся поднимается терраформом за один раз. Что такого использует Jenkins, для получения доступа к кластеру, чего не может модуль Графаны?  